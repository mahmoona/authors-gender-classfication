{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "## Authors's Gender Biopic Text Classification \n",
        "\n",
        "> Wikipedia Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "This notebook demonstrates text classification starting from plain text files stored on disk. I'll train a binary classifier to perform Gender classfication on Wikipedia's authors biopic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-tTFS04dChr",
        "outputId": "41ba762c-c9e8-4603-dddd-5d7422ee9cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBTI1bi8qdFV"
      },
      "source": [
        "\n",
        "Gender Classfication\n",
        "\n",
        "This notebook trains a model to classify the author's biopic either male or female.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "source": [
        "### Import and export data\n",
        "\n",
        "Import text dataset extract the dataset, then explore the directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7ZYnuajVlFN"
      },
      "outputs": [],
      "source": [
        "url = \"sample_data/gender bio.zip\"\n",
        "import zipfile\n",
        "path_to_zip_file = url\n",
        "directory_to_extract_to = \"sample_data\"\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "dataset = \"sample_data\"\n",
        "\n",
        "dataset_dir = \"sample_data/gender bio\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "355CfOvsV1pl",
        "outputId": "3d6ef3ee-9e73-463a-ee2d-3a5e68af6383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data/gender bio\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'train']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "print(dataset_dir)\n",
        "os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ASND15oXpF1",
        "outputId": "3cf97b2f-4dd1-47c8-c9d4-2ddc02f33a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['female bio', 'male bio']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysMNMI1CWDFD"
      },
      "source": [
        "The `gender bio/train/female` and `gender bio/train/male` directories contain many text files, each of which is a single author's biopic. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7g8hFvzWLIZ",
        "outputId": "30b4d7e7-3fab-4d18-ce46-8edbfd2bb9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aaron Dale Allston (December 8, 1960 â€“ February 27, 2014) was an American game designer and author of many science fiction books, notably Star Wars novels.[1] His works as a game designer include game supplements for role-playing games, several of which served to establish the basis for products and subsequent development of TSR's Dungeons & Dragons game setting Mystara. His later works as a novelist include those of the X-Wing series: Wraith Squadron, Iron Fist, Solo Command, Starfighters of Adumar, and Mercy Kill. He wrote two entries in the New Jedi Order series: Enemy Lines I: Rebel Dream and Enemy Lines II: Rebel Stand. Allston wrote three of the nine Legacy of the Force novels: Betrayal, Exile, and Fury, and three of the nine Fate of the Jedi novels: Outcast, Backlash, and Conviction.\n",
            "Allston was born December 8, 1960, in Corsicana, Texas, to Tom Dale Allston and Rose Binford Boehm.[2][3] Allston moved all over Texas in his youth and graduated from high school in Denton.[4] An avid fan of science fiction from an early age, by high school he was the secretary and reporter for his high school science fiction club.[5] Allston moved to Austin in 1979 and attended the University of Texas.[4]\n",
            "Allston was a circulation manager, assistant editor, and editor of Space Gamer magazine,[6] and by 1983 was a full-time freelance game designer.[7] He served as editor of Space Gamer from issues 52 (June 1982) to 65 (September/October 1983),[citation needed] and as editor of Fantasy Gamer for the first issue (August/September 1983) and co-editor of the second issue (December/January 1984).[citation needed] During Allston's tenure as editor, Space Gamer won the H.G. Wells Award for Best Professional Role-Playing Magazine in 1982.[6] Allston authored the book Autoduel Champions in 1983, which crossed over Champions by Hero Games and Car Wars by Steve Jackson Games.[8] Allston helped launch the Fantasy Gamer spinoff magazine.[9] He co-wrote the computer game Savage Empire, which Game Player magazine named the Best PC Fantasy RPG in 1990.[6] He authored the Rules Cyclopedia (1991), a revision and compilation for the Dungeons & Dragons game.[10] He branched into fiction, and in the mid-1990s wrote five novels.[7]\n",
            "He began writing for the Star Wars X-Wing series in 1997, when the primary sequence writer Michael Stackpole could not handle the entire workload.[4] Allston produced a new edition of Champions for Hero Games in 2002.[11] In 2006, he launched The Legacy of the Force series with a hardcover entitled Betrayal.[4]\n",
            "In 2005, Allston made his directorial debut on the independent film Deadbacks, which he also wrote and produced.[4] The film went into post-production but was never released.[12]\n",
            "Allston lived in Round Rock, Texas.[13] For a short time, he worked for the Austin American-Statesman newspaper.[7]\n",
            "In early April 2009 Allston had a heart attack and underwent an emergency quadruple bypass surgery,[14] while on the book signing tour for Outcast,[citation needed] the first book in the Fate of the Jedi series.\n",
            "On February 27, 2014, Allston collapsed during an appearance at VisionCon in Branson, Missouri, apparently from heart failure.[15] He died later that day in Springfield, Missouri, at the age of 53.[3][15]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_file = os.path.join(train_dir, 'male bio/Aaron Allston.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk20TEm6ZRFP"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Next, I will load the data off disk and prepare it into a format suitable for training. To do so, I  will use the helpful text_dataset_from_directory utility, which expects a directory structure as follows.\n",
        "\n",
        "```\n",
        "main_directory/\n",
        "...class_a/\n",
        "......a_text_1.txt\n",
        "......a_text_2.txt\n",
        "...class_b/\n",
        "......b_text_1.txt\n",
        "......b_text_2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95kkUdRoaeMw"
      },
      "source": [
        "*Next*, I will use the `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`. tf.data is a powerful collection of tools for working with data. \n",
        "\n",
        "When running a machine learning experiment, it is a best practice to divide  dataset into three splits:train, validation, and test.\n",
        "\n",
        "I created a validation set using an 80:20 split of the training data by using the `validation_split` argument below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOrK-MTYaw3C",
        "outputId": "e85bcc9d-8bea-4d26-ab4d-6684fa67dbdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Using 1600 files for training.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'sample_data/gender bio/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='training', \n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y33oxOUpYkh"
      },
      "source": [
        "As you can see above, there are 2000 examples in the training folder, of which you will use 80% (or 1600) for training. As you will see in a moment, you can train a model by passing a dataset directly to `model.fit`. I iterate over the dataset and print out a few examples as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51wNaPPApk1K",
        "outputId": "7a256c09-17a9-4b90-c8df-50257eb4e632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biopic b'Thomas H. Bender is an American historian, specializing in urban history and intellectual history. He joined New York University in 1974 and served there as University Professor of the Humanities from 1982 until his retirement.[1][2] He contributes regularly to the press, with articles published in the New York Times, The Nation, Los Angeles Times, Chronicle of Higher Education, and Newsday, among others. He retired in May 2015.\\r\\nHe graduated from Santa Clara University with a B.A. (1966) and the University of California, Davis with an M.A. (1967) and a Ph.D. (1971). He taught Urban Studies and History for three years at the University of Wisconsin\\xe2\\x80\\x93Green Bay from 1971 before moving to New York University. During his tenure there, he was Chair at the Department of History from 1986 to 1989, and Dean for the Humanities from 1995 to 1998.\\r\\nHe moderated an online discussion at History Matters.[3]\\r\\n'\n",
            "Label 1\n",
            "biopic b'\\r\\nNatalie Clifford Barney (October 31, 1876 \\xe2\\x80\\x93 February 2, 1972) was an American writer who hosted a literary salon at her home in Paris that brought together French and international writers. She influenced other authors through her salon and also with her poetry, plays, and epigrams, often thematically tied to her lesbianism and feminism.\\r\\nBarney was born into a wealthy family. She was partly educated in France, and expressed a desire from a young age to live openly as a lesbian. She moved to France with her first romantic partner, Eva Palmer. Inspired by the work of Sappho, Barney began publishing love poems to women under her own name as early as 1900. Writing in both French and English, she supported feminism and pacifism. She opposed monogamy and had many overlapping long and short-term relationships, including on-and-off romances with poet Ren\\xc3\\xa9e Vivien and courtesan Liane de Pougy and longer relationships with writer \\xc3\\x89lisabeth de Gramont and painter Romaine Brooks. \\r\\nBarney hosted a salon at her home in Paris for more than 60 years, bringing together writers and artists from around the world, including many leading figures in French, American, and British literature. Attendees of various sexualities expressed themselves and mingled comfortably at the weekly gatherings. She worked to promote writing by women and hosted a \"Women\\'s Academy\" (L\\'Acad\\xc3\\xa9mie des Femmes) in her salon as a response to the all-male French Academy. The salon closed for the duration of World War II while Barney lived in Italy with Brooks. She initially espoused some pro-fascist views, but supported the Allies by the end of the war. After the war, she returned to Paris, resumed the salon, and continued influencing or inspiring writers such as Truman Capote.\\r\\nBarney had a wide literary influence. Remy de Gourmont addressed public letters to her using the nickname l\\'Amazon (the Amazon), and Barney\\'s association with both de Gourmont and the nickname lasted until her death. Her life and love affairs served as inspiration for many novels written by others, ranging from de Pougy\\'s erotic French bestseller Idylle Saphique to Radclyffe Hall\\'s The Well of Loneliness, the most famous lesbian novel of the twentieth century.[1]\\r\\nBarney was born in 1876 in Dayton, Ohio, to Albert Clifford Barney and Alice Pike Barney.[3] Alice learned to love the arts from her father, who owned Pike\\'s Opera House in Cincinnati, Ohio.[4] Albert Barney partially inherited his family\\'s railroad car manufacturing company, Barney & Smith Car Works.[5]\\r\\nWhen Barney was five years old, she encountered Oscar Wilde at a New York hotel.  Wilde scooped her up as she ran past him fleeing a group of small boys and held her out of their reach. Then he sat her down on his knee and told her a story.[6] The next day he joined Barney and her mother on the beach, and Wilde inspired Alice to pursue art seriously, which she did despite her husband\\'s disapproval.[7]\\r\\nLike many girls of her time, Barney had a haphazard education.[8] Her interest in the French language began with a governess who read Jules Verne stories aloud to her, so she would have to learn quickly to understand them.[9] She and her younger sister Laura attended Les Ruches, a French boarding school in Fontainebleau, France, founded by feminist Marie Souvestre.[10] As an adult, she spoke and wrote French fluently.[11]\\r\\nWhen she was ten, her family moved from Ohio to the Scott Circle area of Washington D.C., spending summers at their large cottage in Bar Harbor, Maine.[3]  As the rebellious and unconventional daughter of one of the wealthiest families in town, she was often mentioned in Washington newspapers. In her early twenties she made headlines by galloping through Bar Harbor while driving a second horse on a lead ahead of her, riding astride instead of sidesaddle.[12]\\r\\nBarney later said she knew she was a lesbian by age twelve,[13] and she was determined to \"live openly, without hiding anything\".[14]\\r\\nBarney\\'s earliest intimate relationship was with Eva Palmer. They became acquainted during summer vacations in Bar Harbor, Maine, and began a sexual relationship during one such trip in 1893. Barney likened Palmer\\'s appearance to that of a medieval virgin.[15] The two remained close for several years.  As young adults in Paris they shared an apartment at 4 rue Chalgrin and eventually took their own residences in Neuilly.[16]  Barney frequently solicited Palmer\\'s help in her romantic pursuits of other women, including Pauline Tarn.[17]  Palmer ultimately left Barney\\'s side for Greece and eventually married Angelos Sikelianos. Their relationship did not survive this turn of events: Barney took a dim view of Angelos and heated letters were exchanged.[18]  Later in their lives the friendship was repaired through correspondence and reunions in New York.[19]\\r\\nIn 1899, after seeing the courtesan Liane de Pougy at a dance hall in Paris, Barney presented herself at de Pougy\\'s residence in a page costume and announced she was a \"page of love\" sent by Sappho.[20] Although de Pougy was one of the most famous women in France, constantly sought after by wealthy and titled men, Barney\\'s audacity charmed her.[21]\\r\\nBarney stood to inherit some family wealth held in trust if she either married or waited for her father\\'s death.[22] While courting de Pougy, Barney was engaged to Robert Cassat, a member of another wealth railroad family.[23] Barney was open with Cassat about her love of women and relationship with de Pougy.[22] In the hopes of securing the Barney trust money, the three briefly considered a rushed wedding between Barney and Cassat and an adoption of de Pougy.[24] When Cassat ended the engagement, Barney attempted unsuccessfully to persuade her father to give her the money anyway.[25]\\r\\nBy the end of 1899, the two had broken up after quarreling repeatedly over Barney\\'s desire to \"rescue\" de Pougy from her life as a courtesan.[26] Despite the breakup, the two continued having liaisons for decades.[27]\\r\\nTheir on-and-off affair became the subject of de Pougy\\'s tell-all roman \\xc3\\xa0 clef, Idylle Saphique (Sapphic Idyll). Published in 1901, the book and its sexually suggestive scenes became the talk of Paris, reprinted more than 70 times in its first year.[28] Barney was soon well known as the model for one of the characters.[29]\\r\\nBarney herself contributed a chapter to Idylle Saphique in which she described reclining at de Pougy\\'s feet in a screened box at the theater, watching Sarah Bernhardt\\'s play Hamlet.[30] During intermission, Barney (as \"Flossie\") compares Hamlet\\'s plight with that of women: \"What is there for women who feel the passion for action when pitiless Destiny holds them in chains? Destiny made us women at a time when the law of men is the only law that is recognized.\"[31]  She also wrote Lettres \\xc3\\xa0 une Connue (Letters to a Woman I Have Known), her own epistolary novel about the affair. Although Barney failed to find a publisher for the book and later called it na\\xc3\\xafve and clumsy, it is notable for its discussion of homosexuality, which Barney regarded as natural and compared to albinism.[32]  \"My queerness,\" she said, \"is not a vice, is not deliberate, and harms no one.\"[33]\\r\\nIn November 1899, Barney met the poet Pauline Tarn, better known by her pen name Ren\\xc3\\xa9e Vivien. For Vivien it was love at first sight, while Barney became fascinated with Vivien after hearing her recite one of her poems,[34] which Barney described as \"haunted by the desire for death\".[35] Their romantic relationship was also a creative exchange that inspired both of them to write. Barney provided a feminist theoretical framework which Vivien explored in her poetry. They adapted the imagery of the Symbolist poets along with the conventions of courtly love to describe love between women, also finding examples of heroic women in history and myth.[36] Sappho was an especially important influence and they studied Greek so as to read the surviving fragments of her poetry in the original. Both wrote plays about her life.[37]\\r\\nVivien saw Barney as a muse and as Barney put it, \"she had found new inspiration through me, almost without knowing me\". Barney felt Vivien had cast her as a femme fatale and that she wanted \"to lose herself\\xc2\\xa0... entirely in suffering\" for the sake of her art.[38] Vivien also believed in fidelity, which Barney was unwilling to agree to. While Barney was visiting her family in Washington, D.C. in 1901, Vivien stopped answering her letters. Barney tried to get her back for years, at one point persuading a friend, operatic mezzo-soprano Emma Calv\\xc3\\xa9, to sing under Vivien\\'s window so she could throw a poem (wrapped around a bouquet of flowers) up to Vivien on her balcony. Both flowers and poem were intercepted and returned by a governess.[39]\\r\\nIn 1904 she wrote Je Me Souviens (I Remember), an intensely personal prose poem about their relationship which was presented as a single handwritten copy to Vivien in an attempt to win her back. They reconciled and traveled together to Lesbos, where they lived happily together for a short time and discussed starting a school of poetry for women like the one which Sappho, according to tradition, had founded on Lesbos some 2,500 years before. However, Vivien soon got a letter from her lover Baroness H\\xc3\\xa9l\\xc3\\xa8ne van Zuylen and went to Constantinople thinking she would break up with her in person. Vivien planned to meet Barney in Paris afterward, but instead stayed with the Baroness. This time, the breakup was permanent.[39]\\r\\nVivien\\'s health declined rapidly after this. The author Colette, who herself had an affair with Barney in 1906, was Vivien\\'s friend and neighbor.[40] According to Colette, Vivien ate almost nothing and drank heavily, even rinsing her mouth with perfumed water to hide the smell.[41] Colette\\'s account has led some to call Vivien an anorexic,[42] but this diagnosis did not yet exist at the time. Vivien was also addicted to the sedative chloral hydrate. In 1908 she attempted suicide by overdosing on laudanum[43] and died the following year. In a memoir written fifty years later, Barney said, \"She could not be saved. Her life was a long suicide. Everything turned to dust and ashes in her hands.\"[44]\\r\\nIn 1949, two years after the death of H\\xc3\\xa9l\\xc3\\xa8ne van Zuylen, Barney restored the Ren\\xc3\\xa9e Vivien Prize[45][46][47][48] with a financial grant[49]  under the authority of the Soci\\xc3\\xa9t\\xc3\\xa9 des gens de lettres and took on the chairmanship of the jury in 1950.[50][51][52]\\r\\nBarney purchased and read Opals in 1900, a debut collection of poems by Olive Custance. Responding to the lesbian themes in the poetry, Barney began corresponding with Custance and exchanging poems.[53] The two met in 1901 at Barney and Vivien\\'s home in Paris, and they soon began a short romantic relationship. While Barney\\'s infidelity aggravated Vivien, Custance was also pursuing a relationship with Lord Alfred Douglas, who she would later marry.[54]\\r\\nIn 1900, Barney published her first book, a collection of poems called Quelques Portraits-Sonnets de Femmes (Some Portrait-Sonnets of Women). The poems were written in traditional French verse and a formal, old-fashioned style since Barney did not care for free verse. Quelques Portraits has been described as \"apprentice work\", a classifier which betrays its historical significance. According to biographer Suzanne Rodriguez, the collection\\'s publication meant that Barney became the first woman poet to openly write about the love of women since Sappho.[55] Her mother contributed pastel illustrations of the poems\\' subjects, wholly unaware three of the four women who modelled for her were her daughter\\'s lovers.[56]\\r\\nReviews were generally positive and glossed over the lesbian theme of the poems, some even misrepresenting it. The Washington Mirror said Barney \"writes odes to men\\'s lips and eyes; not like a novice, either\".[57] However, a headline in a society gossip paper cried out \"Sappho Sings in Washington\" and this alerted her father, who bought and destroyed the publisher\\'s remaining stock and printing plates.[58]\\r\\nTo escape her father\\'s sway Barney published her next book, Cinq Petits Dialogues Grecs (Five Short Greek Dialogues, 1901), under the pseudonym Tryph\\xc3\\xa9. The name came from the works of Pierre Lou\\xc3\\xbfs, who helped edit and revise the manuscript. Barney also dedicated the book to him. The first of the dialogues is set in ancient Greece and contains a long description of Sappho, who is \"more faithful in her inconstancy than others in their fidelity\". Another argues for paganism over Christianity.[59] After the death of Barney\\'s father in 1902, his approximately $9 million fortune ($282 million in 2018[60]) was left in trust with annual income to be split equally between Barney, her mother, and her sister. His death and the money freed her from any need to conceal the authorship of her books; she never used a pseudonym again.[61] She considered scandal \"the best way of getting rid of nuisances\" (meaning heterosexual attention from young men).[62]\\r\\nJe Me Souviens was published in 1910, after Vivien\\'s death.[64] That same year, Barney published Actes et Entr\\'actes (Acts and Interludes), a collection of short plays and poems. One of the plays was \\xc3\\x89quivoque (Ambiguity), a revisionist version of the legend of Sappho\\'s death: instead of throwing herself off a cliff for the love of Phaon the sailor, she does so out of grief that Phaon is marrying the woman she loves. The play incorporates quotations from Sappho\\'s fragments, with Barney\\'s own footnotes in Greek, and was performed with ancient Greek-inspired music and dance.[65][66]\\r\\nBarney did not take her poetry as seriously as Vivien did, saying \"if I had one ambition it was to make my life itself into a poem\".[67] Her plays were only performed through amateur productions in her garden. According to Karla Jay, most of them lacked coherent plots and \"would probably baffle even the most sympathetic audience\".[68] After 1910 she mostly wrote the epigrams and memoirs, for which she is better known. Her last book of poetry was called Poems & Poemes: Autres Alliances and came out in 1920, bringing together romantic poetry in both French and English. Barney asked Ezra Pound to edit the poems, but ignored his detailed recommendations.[69]\\r\\nFor over 60 years, Barney hosted a literary salon, first in Neuilly but mostly at her home at 20, Rue Jacob, in Paris.[70][71] Her salon was a weekly, Friday gathering at which people met to socialize and discuss literature, art, music and any other topic of interest. Though she hosted some of the most prominent male writers of her time, Barney strove to shed light on female writers and their work.[72]\\r\\nIn addition to its focus on women, Barney\\'s salon was distinguished by its deliberately international character.[73][74] She brought together expatriate Modernists with members of the French Academy.[75] The salon Biographer Joan Schenkar described Barney\\'s salon as \"a place where lesbian assignations and appointments with academics could coexist in a kind of cheerful, cross-pollinating, cognitive dissonance\".[76] The range of sexualities welcomed at the salon was also uncommon in Paris, and Barney\\'s openness with her own sexuality made her salon comfortable to homosexual or bisexual attendees.[77]\\r\\nIn the 1900s Barney held early gatherings of the salon at her house in Neuilly. The entertainment included poetry readings and theatricals (in which Colette sometimes performed). Mata Hari performed a dance once, riding into the garden naked as Lady Godiva on a white horse harnessed with turquoise cloisonn\\xc3\\xa9.[78] The play Equivoque may have led Barney to leave Neuilly in 1909. According to a contemporary newspaper article, her landlord objected to her holding an outdoor performance of a play about Sappho, which he felt \"followed nature too closely\".[79] She canceled her lease and rented the pavilion on Rue Jacob in Paris\\'s Latin Quarter, and her salon was held there until the late 1960s. This was a small two-story house, separated on three sides from the main building on the street. Next to the pavilion was a large, overgrown garden with a Doric \"Temple of Friendship\" tucked into one corner. In this new location, the salon grew a more prim outward face, with poetry readings and conversation, perhaps because Barney had been told the pavilion\\'s floors would not hold up to large dancing parties.[80][81] Frequent guests during this period included poets Pierre Lou\\xc3\\xbfs and Paul Claudel, diplomat Philippe Berthelot and translator J. C. Mardrus.[82]\\r\\nDuring World War I, the salon became a haven for those opposed to the war. Henri Barbusse gave a reading from his anti-war novel Under Fire and Barney hosted a Women\\'s Congress for Peace. Other visitors to the salon during the war included Oscar Milosz, Auguste Rodin and poet Alan Seeger, who came while on leave from the French Foreign Legion.[83]\\r\\nEzra Pound was a close friend of Barney\\'s and often visited. The two schemed together to subsidize Paul Val\\xc3\\xa9ry and T. S. Eliot so they could quit their jobs and focus on writing, but Val\\xc3\\xa9ry found other patrons and Eliot refused the grant.  Pound introduced Barney to avant-garde composer George Antheil, and, while her own taste in music leaned towards the traditional, she hosted premieres of Antheil\\'s Symphony for Five Instruments and First String Quartet.[84] It was also at Barney\\'s salon that Pound met his longtime mistress, the violinist Olga Rudge.[85]\\r\\nIn 1927 Barney started an Acad\\xc3\\xa9mie des Femmes (Women\\'s Academy) to honor women writers. This was a response to the influential Acad\\xc3\\xa9mie Fran\\xc3\\xa7aise (French Academy) which had been founded in the 17th century by Louis XIII and whose 40 members included no women at the time. Unlike the French Academy, Barney\\'s was not a formal organization but rather a series of readings held as part of the regular Friday salons. Honorees included Colette, Gertrude Stein, Anna Wickham, Rachilde, Lucie Delarue-Mardrus, Mina Loy, Djuna Barnes and posthumously, Ren\\xc3\\xa9e Vivien.[86] The academy\\'s activities wound down after 1927.[87]\\r\\nOther visitors to the salon during the 1920s included French writers Jeanne Galzy,[88] Andr\\xc3\\xa9 Gide, Anatole France, Max Jacob, Louis Aragon and Jean Cocteau. English-language writers also visited, including Ford Madox Ford, W. Somerset Maugham, F. Scott Fitzgerald, Sinclair Lewis, Sherwood Anderson, Thornton Wilder, T. S. Eliot and William Carlos Williams. Barney also hosted German poet Rainer Maria Rilke, Bengali poet Rabindranath Tagore (the first Nobel laureate from Asia), Romanian aesthetician and diplomat Matila Ghyka, journalist Janet Flanner (also known as Gen\\xc3\\xaat, who set the New Yorker style), journalist, activist and publisher Nancy Cunard, publishers Caresse and Harry Crosby, publisher Blanche Knopf,[89] art collector and patron Peggy Guggenheim, Sylvia Beach (the bookstore owner who published James Joyce\\'s Ulysses), painters Tamara de Lempicka and Marie Laurencin and dancer Isadora Duncan.[90]\\r\\nFor her 1929 book Aventures de l\\'Esprit (Adventures of the Mind) Barney drew a social diagram which crowded the names of over a hundred people who had attended the salon into a rough map of the house, garden and Temple of Friendship. The first half of the book had reminiscences of 13 male writers she had known or met over the years and the second half had a chapter for each member of her Acad\\xc3\\xa9mie des Femmes.[91]\\r\\nIn the late 1920s Radclyffe Hall drew a crowd reading her novel The Well of Loneliness, recently banned in the UK.[92] A reading by poet Edna St. Vincent Millay packed the salon in 1932. At another Friday salon in the 1930s Virgil Thomson sang from Four Saints in Three Acts, an opera based on a libretto by Stein.[93]\\r\\nOf the famous Modernist writers who spent time in Paris, Ernest Hemingway never made an appearance at the salon. James Joyce came once or twice but did not care for it. Marcel Proust never attended a Friday but did come once to talk with Barney about lesbian culture whilst doing research for In Search of Lost Time, though he ended up too nervous to bring up the subject.[94]\\r\\n\\xc3\\x89parpillements (Scatterings, 1910) was Barney\\'s first collection of pens\\xc3\\xa9es\\xe2\\x80\\x94literally, thoughts.  This literary form had been associated with salon culture in France since the 17th century, when the genre was perfected at the salon of Madame de Sabl\\xc3\\xa9.[95] Barney\\'s pens\\xc3\\xa9es, like de Sabl\\xc3\\xa9\\'s own Maximes, were short, often one-line epigrams or bon mots such as \"There are more evil ears than bad mouths\" and \"To be married is to be neither alone nor together.\"[96]\\r\\nHer literary career got a boost after she sent a copy of \\xc3\\x89parpillements to Remy de Gourmont, a French poet, literary critic, and philosopher who had become a recluse after contracting the disfiguring disease lupus vulgaris in his thirties.[97] He was impressed enough to invite her to one of the Sunday gatherings at his home, at which he usually received only a small group of old friends.  She was a rejuvenating influence in his life, coaxing him out for evening car rides, dinners at the Rue Jacob, a masked ball, even a short cruise on the Seine.  He turned some of their wide-ranging conversations into a series of letters that he published in the Mercure de France, addressing her as l\\'Amazone, a French word that can mean either horsewoman or Amazon; the letters were later collected in book form.  He died in 1915, but the nickname he gave her would stay with her all her life\\xe2\\x80\\x94even her tombstone identifies her as \"the Amazon of Remy de Gourmont\"\\xe2\\x80\\x94and his Letters to the Amazon left readers wanting to know more about the woman who had inspired them.[98]\\r\\nBarney obliged in 1920 with Pens\\xc3\\xa9es d\\'une Amazone (Thoughts of an Amazon), her most overtly political work.  In the first section, \"Sexual Adversity, War, and Feminism\", she developed feminist and pacifist themes, describing war as an \"involuntary and collective suicide ordained by man\".[99] In war, she said, men \"father death as women mother life, with courage and without choice\".[100] The epigrammatic form makes it difficult to determine the details of Barney\\'s views; ideas are presented only to be dropped, and some pens\\xc3\\xa9es seem to contradict others.[101] Some critics interpret her as saying that the aggression that leads to war is visible in all male relationships.  Karla Jay, however, argues that her philosophy was not that sweeping, and is better summed up by the epigram \"Those who love war lack the love of an adequate sport\\xe2\\x80\\x94the art of living.\"[102][99][100]\\r\\nAnother section of Pens\\xc3\\xa9es d\\'une Amazone, \"Misunderstanding, or Sappho\\'s Lawsuit\", gathered historical writings about homosexuality along with her own commentary.[103] She also covered topics such as alcohol, friendship, old age, and literature, writing \"Novels are longer than life\"[104] and \"Romanticism is a childhood ailment; those who had it young are the most robust.\"[105] A third volume, Nouvelles Pens\\xc3\\xa9es de l\\'Amazone (New Thoughts of the Amazon), appeared in 1939.\\r\\nThe One Who is Legion, or A.D.\\'s After-Life (1930) was Barney\\'s only book written entirely in English, as well as her only novel.  Illustrated by Romaine Brooks, it concerns a person who committed suicide, known only as A.D., who is brought back to life as a genderless, hermaphroditic being and reads the book of their own life.  This book-within-a-book, entitled The Love-Lives of A.D., is a collection of hymns, poems and epigrams, much like Barney\\'s own other writings.[106][107]\\r\\nDespite several of her lovers\\' objections, Barney practiced, and advocated, non-monogamy. As early as 1901, in Cinq Petits Dialogues Grecs, she argued in favor of multiple relationships and against jealousy;[108] in \\xc3\\x89parpillements she wrote \"One is unfaithful to those one loves in order that their charm does not become mere habit\".[109] While she could be quite jealous herself, she actively encouraged at least some of her lovers to be non-monogamous as well.[110]\\r\\nDue in part to Jean Chalon\\'s early biography of her, published in English as Portrait of a Seductress, Barney had become more widely known for her many relationships than for her writing or her salon.[111] She once wrote out a list, divided into three categories: liaisons, demi-liaisons, and adventures. Colette was a demi-liaison, while the artist and furniture designer Eyre de Lanux, with whom she had an off-and-on affair for several years, was listed as an adventure.  Among the liaisons\\xe2\\x80\\x94the relationships that she considered most important\\xe2\\x80\\x94were Custance, Vivien, \\xc3\\x89lisabeth de Gramont, Brooks, and Dolly Wilde.[112] Many of her affairs, like those with Colette and Lucie Delarue-Mardrus, evolved into lifelong friendships.[113]\\r\\n\\xc3\\x89lisabeth de Gramont, the Duchess of Clermont-Tonnerre, was a writer best known for her popular memoirs. A descendant of Henry IV of France, she had grown up among the aristocracy; when she was a child, according to Janet Flanner, \"peasants on her farm\\xc2\\xa0... begged her not to clean her shoes before entering their houses\".[114] Her father\\'s ancestors had squandered their fortune and he married into the Rothschild family after her birth; she did not have any access to her step-mother\\'s wealth.[115] She looked back on this lost world of wealth and privilege with little regret, and became known as the \"red duchess\" for her support of socialism. Encouraged by her father to wed into security, she married Philibert de Clermont-Tonnere and had two daughters. He was violent and tyrannical.[116][117]\\r\\nThe poet Lucie Delarue-Mardrus introduced Barney and de Gramont in 1909 or 1910.[118][117][119] The couple shared academic interests and attended Remy de Gourmont\\'s salon together.[120]\\r\\nBarney wrote an unpublished novel inspired by their early relationship, L\\xe2\\x80\\x99Adult\\xc3\\xa8re ing\\xc3\\xa9nue (The Adulterous Ing\\xc3\\xa9nue).[121]\\r\\nDe Gramont accepted Barney\\'s nonmonogamy\\xe2\\x80\\x94perhaps reluctantly at first\\xe2\\x80\\x94and went out of her way to be gracious to her other lovers,[122] always including Brooks when she invited Barney to vacation in the country.[123]\\r\\nThough the two conducted their affair clandestinely, de Gramont\\'s husband found them out and attempted to stop them from seeing each other.[120]  He was unsuccessful, and he divorced de Gramont in 1920 after a period of separation.[124] In 1918 she and Barney wrote up a marriage contract stating: \"No one union shall be so strong as this union, nor another joining so tender\\xe2\\x80\\x94nor relationship so lasting\".[125] The relationship continued until de Gramont\\'s death in 1954.[126]\\r\\nBarney\\'s longest relationship was with the American painter Romaine Brooks, whom she met around 1915.[127]  Brooks specialized in portraiture and was noted for her somber palette of gray, black, and white.[128]  During the 1920s she painted portraits of several members of Barney\\'s social circle, including de Gramont and Barney herself.[129]\\r\\nBrooks tolerated Barney\\'s casual affairs well enough to tease her about them, and had a few of her own over the years, but could become jealous when a new love became serious.  Usually she simply left town, but at one point she gave Barney an ultimatum to choose between her and Dolly Wilde\\xe2\\x80\\x94relenting once Barney had given in.[130] At the same time, while Brooks was devoted to Barney, she did not want to live with her as a full-time couple; she disliked Paris, disdained Barney\\'s friends, hated the constant socializing on which Barney thrived, and felt that she was fully herself only when alone.[131][132] To accommodate Brooks\\'s need for solitude they built a summer home consisting of two separate wings joined by a dining room, which they called Villa Trait d\\'Union, the hyphenated villa.  Brooks also spent much of the year in Italy or travelling elsewhere in Europe, away from Barney.[133] Their relationship lasted for over fifty years.[134]\\r\\nDolly Wilde was the niece of Oscar Wilde and the last of her family to bear the Wilde name.  She was renowned for her epigrammatic wit but, unlike her famous uncle, never managed to apply her gifts to any publishable writing; her letters are her only legacy.  She did some work as a translator and was often supported by others, including Barney, whom she met in 1927.[135]\\r\\nBarney\\'s support of Wilde included occasional permission to stay for a few weeks at Rue Jacob. Brooks\\' disapproval of the relationship increased over the years, aggravated by Wilde\\'s presence in Barney\\'s home. Wilde, the only of Barney\\'s loves to share her enthusiastic rejection of monogamy,[136] strove conscientiously but futilely for Brooks\\' favor. This culminated in Brooks\\' ultimatum, delivered in 1931, in which she described Wilde as a rat \"gnawing at the very foundation of our friendship\".[137] Barney chose Brooks and separated from Wilde; Brooks later allowed Wilde to return and became less critical of Wilde\\'s ways.[138]\\r\\nLike Vivien, Wilde was intensely self-destructive and struggled deeply with mental illness. She attempted suicide several times, and spent much of her life addicted to alcohol and heroin. Barney, a vocal opponent of drug use and alcoholism, financed drug detoxifications several times; to no avail. Wilde even emerged from one nursing-home stay with a new dependency on the sleeping draught paraldehyde, then available over-the-counter.[139]\\r\\nIn 1939, she was diagnosed with breast cancer and refused surgery, seeking alternative treatments.[140] The following year, World War II separated her from Barney; she fled Paris for England while Barney went to Italy with Brooks.[141] She died in 1941 from causes never fully explained; with one of the most common speculations being a paraldehyde overdose.[142] Her will, written in 1932, named Barney as her only heir.[143]\\r\\nBarney\\'s attitudes during World War II have been controversial.  In 1937, Una, Lady Troubridge, complained that Barney \"talked a lot of half-baked nonsense about the tyranny of fascism\".[144] Barney herself had Jewish heritage,[145] and since she spent the war in Florence with Brooks, was investigated by Italian authorities because of this; she was able to escape their attention after her sister Laura arranged for a notarized document attesting to her confirmation.[146] Nevertheless, she believed Axis propaganda that portrayed the Allies as the aggressors, so that pro-Fascism seemed to her to be a logical consequence of her pacifism.  An unpublished memoir she wrote during the war years is pro-Fascist and anti-Semitic, quoting speeches by Hitler, apparently with approval.[147]\\r\\nIt is possible that the anti-Semitic passages in her memoir were intended to be used as evidence that she was not Jewish;[148] alternatively, she may have been influenced by Ezra Pound\\'s anti-Semitic radio broadcasts.[149] Whatever the case, she did help a Jewish couple escape Italy, providing passage on a ship to the United States.[147] By the end of the war her sympathies had again changed, and she saw the Allies as liberators.[150]\\r\\nVilla Trait d\\'Union was destroyed by bombing.  After the war, Brooks declined to live with Barney in Paris; she remained in Italy, and they visited each other frequently.[151] Their relationship remained mostly monogamous until the mid-1950s, when Barney met her last new love, Janine Lahovary, the wife of a retired Romanian ambassador.  Lahovary made a point of winning Brooks\\'s friendship, Barney reassured Brooks that their relationship still came first, and the triangle appeared to be stable.[152]\\r\\nThe salon resumed in 1949 and continued to attract young writers for whom it was as much a piece of history as a place where literary reputations were made.  Truman Capote was an intermittent guest for almost ten years; he described the decor as \"totally turn-of-the-century\" and remembered that Barney introduced him to the models for several characters in Marcel Proust\\'s In Search of Lost Time.[153] Alice B. Toklas became a regular after her partner Stein\\'s death in 1946.  Fridays in the 1960s honored Mary McCarthy and Marguerite Yourcenar, who in 1980\\xe2\\x80\\x94eight years after Barney\\'s death\\xe2\\x80\\x94became the first female member of the French Academy.[154]\\r\\nBarney did not return to writing epigrams, but did publish two volumes of memoirs about other writers she had known, Souvenirs Indiscrets (Indiscreet Memories, 1960) and Traits et Portraits (Traits and Portraits, 1963).  She also worked to find a publisher for Brooks\\'s memoirs and to place her paintings in galleries.[155]\\r\\nIn the late 1960s Brooks became increasingly reclusive and paranoid; she sank into a depression and refused to see the doctors Barney sent.  Bitter at Lahovary\\'s presence during their last years, which she had hoped they would spend exclusively together, she finally broke off contact with Barney.  Barney continued to write to her, but received no replies.  Brooks died in December 1970, and Barney on February 2, 1972, of heart failure.[156] She is buried at Passy Cemetery, Paris, \\xc3\\x8ele-de-France, France.[157] She left some of her writing, including more than 40,000 letters, to the Biblioth\\xc3\\xa8que litt\\xc3\\xa9raire Jacques-Doucet in Paris.[158]\\r\\nBy the end of Natalie Barney\\'s life her work had been largely forgotten. In 1979, Barney was honored with a place setting in Judy Chicago\\'s feminist work of art The Dinner Party. In the 1980s Barney began to be recognized for what Karla Jay calls an \"almost uncanny anticipation\" of the concerns of later feminist writers.[159] English translations of some of her memoirs, essays, and epigrams appeared in 1992, but most of her plays and poetry are untranslated.\\r\\nHer indirect influence on literature, through her salon and her many literary friendships, can be seen in the number of writers who have addressed or portrayed her in their works.  Claudine s\\'en va (Claudine and Annie, 1903) by Colette contains a brief appearance by Barney as \"Miss Flossie\",[160] echoing the nickname she had earlier been given in de Pougy\\'s novel Idylle Saphique.  Ren\\xc3\\xa9e Vivien wrote many poems about her, as well as a Symbolist novel, Une femme m\\'apparut (A Woman Appeared to Me, 1904), in which Barney is described as having \"eyes\\xc2\\xa0... as sharp and blue as a blade\\xc2\\xa0... The charm of peril emanated from her and drew me inexorably.\"[161] Remy de Gourmont addressed her in his Letters to the Amazon, and Truman Capote mentioned her in his last, unfinished novel Answered Prayers.  She also appeared in later novels by writers who never met her. Anna Livia\\'s Minimax (1991) portrays both Barney and Ren\\xc3\\xa9e Vivien as still-living vampires. Francesco Rapazzini\\'s Un soir chez l\\'Amazone (2001) is a historical novel about Barney\\'s salon. The English translation by Sally Hamilton and Suzanne Stroh was published as an audiobook read by Suzanne Stroh under the title A Night at the Amazon\\'s (2020).\\r\\nBarney appears in Hall\\'s The Well of Loneliness as the salon hostess Val\\xc3\\xa9rie Seymour, a symbol of self-acceptance in contrast with the protagonist\\'s self hatred.[162][163] Hall wrote: \"Val\\xc3\\xa9rie, placid and self-assured, created an atmosphere of courage; everyone felt very normal and brave when they gathered together at Val\\xc3\\xa9rie Seymour\\'s.\"[164] According to Lillian Faderman, \"There was probably no lesbian in the four decades between 1928 and the late 1960s capable of reading English or any of the eleven languages into which the book was translated who was unfamiliar with The Well of Loneliness.\"[165]\\r\\nLucie Delarue-Mardrus wrote love poems to Barney in the early years of the century, and in 1930 depicted her in a novel, L\\'Ange et les Pervers (The Angel and the Perverts), in which she said she \"analyzed and described Natalie at length as well as the life into which she initiated me\".  The protagonist of the novel is a hermaphrodite named Marion who lives a double life, frequenting literary salons in female dress, then changing from skirt to trousers to attend gay soir\\xc3\\xa9es. Barney is Laurette Wells, a salon hostess who spends much of the novel trying to win back an ex-lover loosely based on Ren\\xc3\\xa9e Vivien.[166] The book\\'s portrayal of her is, at times, harshly critical, but she is the only person whose company Marion enjoys.  Marion tells Wells that she is \"perverse\\xc2\\xa0... dissolute, self-centered, unfair, stubborn, sometimes miserly\\xc2\\xa0... [but] a genuine rebel, ever ready to incite others to rebellion\\xc2\\xa0.... [Y]ou are capable of loving someone just as they are, even a thief\\xe2\\x80\\x94in that lies your only fidelity.  And so you have my respect.\"[167]\\r\\nAfter meeting Barney in the 1930s, the Russian poet Marina Ivanovna Tsvetaeva addressed her in a Letter to the Amazon (1934) in which she expressed her conflicted feelings about love between women.  The result, according to Terry Castle, is \"an entirely cryptic, paranoid, overwhelming piece of reverie\".[168]\\r\\nBarney and the women in her social circle are the subject of Djuna Barne\\'s Ladies Almanack (1928), a roman \\xc3\\xa0 clef written in an archaic, Rabelaisian style, with Barnes\\' own illustrations in the style of Elizabethan woodcuts.  She has the lead role as Dame Evangeline Musset, \"who was in her Heart one Grand Red Cross for the Pursuance, the Relief and the Distraction, of such Girls as in their Hinder Parts, and their Fore Parts, and in whatsoever Parts did suffer them most, lament Cruelly\".[169]  \"[A] Pioneer and a Menace\" in her youth, Dame Musset has reached \"a witty and learned Fifty\";[170] she rescues women in distress, dispenses wisdom, and upon her death is elevated to sainthood.  Also appearing pseudonymously are de Gramont, Brooks, Dolly Wilde, Hall and her partner Una, Lady Troubridge, Janet Flanner and Solita Solano, and Mina Loy.[171] The obscure language, inside jokes, and ambiguity of Ladies Almanack have kept critics arguing about whether it is an affectionate satire or a bitter attack, but Barney herself loved the book and reread it throughout her life.[172]\\r\\nOn October 26, 2009, Barney was honored with a historical marker in her home town of Dayton, Ohio. The marker is the first in Ohio to note the sexual orientation of its honoree.[173]\\r\\nBarney\\'s French novel, Amants f\\xc3\\xa9minins ou la troisi\\xc3\\xa8me, believed to have been written in 1926, was published in 2013. It was translated into English by Chelsea Ray and published in 2016 as Women Lovers or The Third Woman.[174]\\r\\n\\r\\n'\n",
            "Label 0\n",
            "biopic b'\\r\\nJanet Lee Carey (born January 11, 1954)[1][2][3] is an American college professor who writes fantasy fiction for children and young adults. Her novels The Dragons of Noor (2010) won a Teens Read Too Gold Star Award for Excellence,  Dragon\\'s Keep (2007) won an ALA Best Books for Young adults, and Wenny Has Wings (2002) won the Mark Twain Award (2005).[4]\\r\\nCarey was born in New York[2] and was raised in Mill Valley, California. Carey moved to Seattle, Washington to be closer to her mother and stepfather, where she currently lives at Seattle, Washington. Carey is a very imaginative person and she finds herself daydreaming continuously throughout the day, which allows her to get into her \"fantasy land\" while writing.  She considers herself a homebody who enjoys reading and spending time with family and friends.[5]\\r\\nMany of Carey\\'s novels involve an ordinary child doing heroic deeds.[6] She believes all children need courage in order to develop into successful adults. Carey says courage requires \"a sense of purpose, a belief that what I do matters, a willingness to sacrifice, and the strength to fail and keep going.\"[6] Her novels show children and teens overcoming struggles and obstacles which eventually influence their development and growth as individuals.[6]\\r\\nJanet Lee Carey is a successful children\\'s literature author as well as a teacher. Her desire to become a writer began at a very young age reading her favorite authors seated high up in the branches of a tree. Although at first she was unable to progress from inspiration to publication, she eventually became a successful writer through her dedication, love of story, and passion for writing.[5] Carey faced years of rejection; however, she kept sending her stories out. She believes that the joys far outweigh the struggles with writing.[5]\\r\\nShe has taught at Lake Washington Technical college, Bellevue college, and she leads professional seminars and workshops designed for children and adults.[7] Carey is involved in many groups including her critique group, The Diviners, and arts group, Artemis. The Diviners is a dynamic critique group which helped her progress as a writer, through the revision and analysis of her work. Artemis is a support group for artists, made up of writers, photographers, painters, collage artists, sculptures and musicians. They meet to openly discuss their goals and successes as well as the difficulties and hardships of being an artist.[5] She is also involved with readergirlz and Society of Children\\'s Book Writers and Illustrators. Readergirlz is an online blog which supports reading for teenage girls.[7] SCBWI is a non-profit organization specifically for those involved with children and young adult literature.[8]  Along with her interest in working with other writers, she is also a strong supporter of charitable organizations and environmental awareness. Carey demonstrates her support by linking each of her novels to a charitable organization, hoping to empower readers to make a difference in the world.[5]\\r\\nBetween finding time for her group work and writing, Carey has also dedicated much of her time to hosting writing retreats for future authors. She tours across the United States and overseas presenting at schools and children\\'s book festivals and conferences.[5]\\r\\nCarey\\'s career as a writer was influenced by many different authors, books, her imagination, and personal struggles. Authors such as Ursula K. Le Guin, Juliet Marillier, Patricia A. McKillip, Shannon Hale, Kristin Cashore and many more constantly inspired her as a writer. The fantasy stories she read as a child \"was the reason [she] wanted to grow up to be a writer.\" She was also inspired by Grimms\\' Fairy Tales, and stories involving myths and fantasies. Carey always had a wild imagination. As a child she believed that trees whispered to her; telling her stories in a language she couldn\\'t understand. Her imagination helps her create vivid fantasy novels. Cary\\'s determination to understand \"things that haunt [her] and keep [her] awake at night,\" also influenced her writing. She began writing Stealing Death when her mother was dying and she addresses the question \"Why do we have to die\" throughout the novel.[5]\\r\\nRosalind is born with an unsightly dragon claw. Those who see the claw do not live long after to tell the story. Rosalind is faced with the realization that she will never find love or marry. She is captured by the Dragon Lord and her adventures begin.[2]\\r\\nWilde Island is in a difficult situation, with the trust between the fairies, dragons and humans, being tested with the death of the king. The island is looking for a hero and young Tess, a blacksmith\\'s daughter, is thrown into the crossfire.[2]\\r\\nWhen a hidden killer slays the royal Pendragon heir, the murder is made to look like an accident, but the queen\\'s healer, Uma, and her ally, Jackrun, sense the darker truth. Together, they must use their combined powers to outwit a secret plot to overthrow the Pendragon throne. But are they strong enough to overcome a murderer aided by prophecy and cloaked in magic? A perfectly crafted story combining medieval history, mythology, and fantasy. \"This is a must-purchase for libraries owning the earlier installments and a great choice for where teen fantasy is popular.\" \\xe2\\x80\\x93School Library Journal [2]\\r\\nA fantasy novel based around a young boy named Kipp who has lost everything of value to him. He is left in charge of his younger sister after the rest of his family was killed in a fire. He is determined to keep those he holds close to him away from Death\\'s soul sack.[2]\\r\\nWill North loses his sister Wenny in a terrible accident. Will writes a series of letters to his sister in heaven. He believes her spirit is in a good place because of the vision he had during his Near Death Experience, but is he brave enough to tell his mother and father what he saw? \"Wenny Has Wings is a powerful, emotional, highly recommended story about learning to cope with grief and loss.\" --Children\\xe2\\x80\\x99s Bookwatch \\xe2\\x80\\x93Midwest Book Review\\r\\n\"A heartrending glimpse into what happens in a family when a child dies.\"\\xe2\\x80\\x94Kirkus Reviews\\r\\nThis novel is about a young girl who is in denial about the fact that her father has died in World War II, until she is given her father\\'s watch. Molly risks it all to find out the truth she is trying to avoid.[2]\\r\\nZoe\\'s family is thrown into troubled times, with her father losing his job and their rental house suddenly being sold. They are forced to move to find work and meanwhile live in the family van. To escape the sad reality, Zoe creates a \"double life\". It is a story of courage and hardships.[2]\\r\\n\"Determined to break the Shriker\\'s curse, Miles steals a spell from his mentor, setting in motion a complex weave of force and power well beyond this world, and drawing Miles into a journey that will require all his courage, and from which only Hanna and her ingenuity can save him. One after the other, the brash, well-intentioned hero and the plucky young heroine enter a realm of betrayal, honor, destiny, and otherworldly justice. Plot twists combine with magic, suspense, legend, challenge, and redemption. [This] will appeal most to dedicated fantasy readers.\" --VOYA\\r\\nTrouble is stirring in the world of Noor. After centuries-long exile, the dragons are uneasy and about to return. A strange wind has been blowing, sweeping children up into the sky. Among the missing is Miles\\'s and Hanna\\'s younger brother. They go after him and find themselves fighting alongside the dragons in a life- altering revolution.[2]\\r\\n'\n",
            "Label 0\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"biopic\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWq1SUIrp1a-"
      },
      "source": [
        "Notice the  author's biopic contain raw text (with punctuation and occasional HTML tags like `<br/>`). You will show how to handle these in the following section. \n",
        "\n",
        "The labels are 0 or 1. To see which of these correspond to female and male aurthor, you can check the `class_names` property on the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlICTG8spyO2",
        "outputId": "52dca30d-370c-4ee5-d157-3067922346e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to female bio\n",
            "Label 1 corresponds to male bio\n"
          ]
        }
      ],
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzxazN8Hq1pF"
      },
      "source": [
        "\n",
        " When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsMwwhOoqjKF",
        "outputId": "0e433694-0ae4-4774-d45f-5cf7b4dff711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Using 400 files for validation.\n"
          ]
        }
      ],
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'sample_data/gender bio/train', \n",
        "    batch_size=batch_size, \n",
        "    validation_split=0.2, \n",
        "    subset='validation', \n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdSr0Nt3q_ns",
        "outputId": "471efd35-032b-4506-e752-d28db30985dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'sample_data/gender bio/test', \n",
        "    batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJmTiO0IYAjm"
      },
      "source": [
        "### Prepare the dataset for training\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "Next, I will standardize, tokenize, and vectorize the data using the helpful `tf.keras.layers.TextVectorization` layer. \n",
        "\n",
        "Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset. Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words, by splitting on whitespace). Vectorization refers to converting tokens into numbers so they can be fed into a neural network. All of these tasks can be accomplished with this layer.\n",
        "\n",
        "As you saw above, the authors biopic contain various HTML tags like `<br />`. These tags are not removed by the default standardizer in the `TextVectorization` layer (which converts text to lowercase and strips punctuation by default, but doesn't strip HTML). I wrote a custom standardization function to remove the HTML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVcHl-SLrH-u"
      },
      "source": [
        "To prevent training-testing skew (also known as training-serving skew), it is important to preprocess the data identically at train and test time. To facilitate this, the `TextVectorization` layer can be included directly inside your model, as shown later in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDRI_s_tX1Hk"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2d3Aw8dsUux"
      },
      "source": [
        "Next, you will create a `TextVectorization` layer. You will use this layer to standardize, tokenize, and vectorize our data. You set the `output_mode` to `int` to create unique integer indices for each token.\n",
        "\n",
        "Note that you're using the default split function, and the custom standardization function you defined above. You'll also define some constants for the model, like an explicit maximum `sequence_length`, which will cause the layer to pad or truncate sequences to exactly `sequence_length` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c76RvSzsMnX"
      },
      "outputs": [],
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlFOpfF6scT6"
      },
      "source": [
        " `adapt` to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAhdjK7AtroA"
      },
      "source": [
        "Note: It's important to only use your training data when calling adapt (using the test set would leak information)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH4_2ZGJsa_X"
      },
      "outputs": [],
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHQVEFzNt-K_"
      },
      "source": [
        "A function to see the result of using this layer to preprocess some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCIg_T50wOCU"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XULcm6B3xQIO",
        "outputId": "98408f7a-6b49-4dcc-a206-4d51f90593e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review tf.Tensor(b\"Deborah Coates is an American author.  She grew up in western New York, and currently lives in Ames, Iowa.[1]  Her stories have been included in Strange Horizons, SCIFICTION, Best American Fantasy 2008, Year's Best Fantasy 6, and Best Paranormal Romance.[2]\\r\\n\\r\\nThis American novelist article is a stub. You can help Wikipedia by expanding it.This article about an American short story writer is a stub. You can help Wikipedia by expanding it.\", shape=(), dtype=string)\n",
            "Label female bio\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[3331,    1,   16,   21,   26,   71,   15,  304,   89,    4,  561,\n",
            "          23,   36,    3,  345,  220,    4, 2598,    1,   12,   74,   56,\n",
            "          48,  279,    4, 1740, 5608,    1,   61,   26,  239,  180,   59,\n",
            "          61,  239,  814,    3,   61, 2799,    1,   45,   26,  179,  199,\n",
            "          16,    6,  369,  101,  150,  233,  364,   20,  355, 2631,  199,\n",
            "          40,   21,   26,   81,   68,   62,   16,    6,  369,  101,  150,\n",
            "         233,  364,   20,  355,   25,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ],
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_author_biopic, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Author's biopic\", first_author_biopic)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized Author's biopic\", vectorize_text(first_author_biopic, first_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u5EX0hxyNZT"
      },
      "source": [
        "As you can see above, each token has been replaced by an integer. You can lookup the token (string) that each integer corresponds to by calling `.get_vocabulary()` on the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRq9hTQzhVhW",
        "outputId": "809228dc-9fcc-4c08-daa6-7dcb04c17f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1287 --->  poets\n",
            " 313 --->  nominated\n",
            "Vocabulary size: 10000\n"
          ]
        }
      ],
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD2H6utRydGv"
      },
      "source": [
        "You are nearly ready to train your model. As a final preprocessing step, you will apply the TextVectorization layer you created earlier to the train, validation, and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zhmpeViI1iG"
      },
      "outputs": [],
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsVQyPMizjuO"
      },
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training  model. If  dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "`.prefetch()` overlaps data preprocessing and model execution while training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMcs_H7izm5m"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "source": [
        "### Create the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkQP6in8yUBR"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpKOoWgu-llD",
        "outputId": "2a21ad3d-bace-4b2b-8ce5-b0b3a228206e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 16)          160016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 16)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "1. The first layer is an `Embedding` layer. This layer takes the integer-encoded reviews and looks up an embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`. \n",
        "2. Next, a `GlobalAveragePooling1D` layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "3. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units. \n",
        "4. The last layer is densely connected with a single output node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), you'll use `losses.BinaryCrossentropy` loss function.\n",
        "\n",
        "Now, configure the model to use an optimizer and a loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Train the model\n",
        "\n",
        "You will train the model by passing the `dataset` object to the fit method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXSGrjWZ-llW",
        "outputId": "17c51e59-2b60-4bd7-b1aa-aeaa106078e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "50/50 [==============================] - 4s 16ms/step - loss: 0.6890 - binary_accuracy: 0.6469 - val_loss: 0.6852 - val_binary_accuracy: 0.7200\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6778 - binary_accuracy: 0.7981 - val_loss: 0.6731 - val_binary_accuracy: 0.8225\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6616 - binary_accuracy: 0.8500 - val_loss: 0.6560 - val_binary_accuracy: 0.8575\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6398 - binary_accuracy: 0.8750 - val_loss: 0.6340 - val_binary_accuracy: 0.8650\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6139 - binary_accuracy: 0.8981 - val_loss: 0.6077 - val_binary_accuracy: 0.8800\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.5838 - binary_accuracy: 0.9169 - val_loss: 0.5782 - val_binary_accuracy: 0.8850\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.5498 - binary_accuracy: 0.9325 - val_loss: 0.5474 - val_binary_accuracy: 0.8875\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.5164 - binary_accuracy: 0.9350 - val_loss: 0.5160 - val_binary_accuracy: 0.9050\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.4819 - binary_accuracy: 0.9413 - val_loss: 0.4848 - val_binary_accuracy: 0.9025\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.4480 - binary_accuracy: 0.9506 - val_loss: 0.4550 - val_binary_accuracy: 0.9100\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "### Evaluate the model\n",
        "\n",
        "Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN",
        "outputId": "edb8e542-d0b4-4651-c1ec-d3ef9b8978d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 7ms/step - loss: 0.4680 - binary_accuracy: 0.9250\n",
            "Loss:  0.4680285155773163\n",
            "Accuracy:  0.925000011920929\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "This fairly naive approach achieves an accuracy of about 92%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldbQqCw2Xc1W"
      },
      "source": [
        "### Create a plot of accuracy and loss over time\n",
        "\n",
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YcvZsdvWfDf",
        "outputId": "9070eca7-22c6-40a9-8d71-8602b327dcf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_CH32qJXruI"
      },
      "source": [
        "There are four entries: one for each monitored metric during training and validation. I Plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SEMeQ5YXs8z",
        "outputId": "9004606a-8764-4dec-bda5-e3015b807bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3PJemLPXwz_",
        "outputId": "76070b27-9071-4ae3-ce9d-ea51f681ac6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFFyCuJoXy7r"
      },
      "source": [
        "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimizationâ€”it should minimize the desired quantity on every iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-to23J3Vy5d3"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Export the model\n",
        "\n",
        "In the code above, you applied the `TextVectorization` layer to the dataset before feeding text to the model. If you want to make your model capable of processing raw strings (for example, to simplify deploying it), you can include the `TextVectorization` layer inside your model. To do so, you can create a new model using the weights you just trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWXsMvryuZuq",
        "outputId": "56c22a5a-6bda-4b9d-8aa5-34e113848a43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 8ms/step - loss: 0.4680 - accuracy: 0.9250\n",
            "0.925000011920929\n"
          ]
        }
      ],
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwQgoN88LoEF"
      },
      "source": [
        "### Inference on new data\n",
        "\n",
        "To get predictions for new examples, you can simply call `model.predict()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW355HH5L49K",
        "outputId": "3a441018-afaf-4994-ccec-e8db7aeab326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4781294 ],\n",
              "       [0.32479757],\n",
              "       [0.4594456 ],\n",
              "       [0.40370232],\n",
              "       [0.4458747 ],\n",
              "       [0.47114527],\n",
              "       [0.7147199 ],\n",
              "       [0.55211544],\n",
              "       [0.5533681 ],\n",
              "       [0.49372795]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "examples = [\n",
        "  \"Martin was born on September 8 in the Bronx borough of New York City, the only child Joseph G. Martin and Josephine DiLorenzo, both journalists who met and married while working at the New York Daily News. Joseph G. Martin had a career that spanned more than 45 years. As a reporter for the New York Daily News, he won nearly every possible honor for reporting: a Selurians Award (1950); two George Polk Awards (1952 and 1973) and a Newspaper Guild Page One Award (1956).[2] In 1959, Martin won a Pulitzer Prize for international reporting for a 10-part series co-written with Philip Santori on Cuban dictator Fulgencio Batista.[3] The piece predicted the fall of Batista months before it happened and was called â€œprophetic journalismâ€ by the Pulitzer committee. Martin also served as New York Deputy Police Commissioner for public relations from 1965 to 1968.[2] Josephine DiLorenzo was raised in the Bronx. A highly gifted student, she graduated high school at 15 and entered Hunter College at 16 years of age. She got a job as one of the first female copyboys at the New York Daily News, working her way up to a reporter. Besides Daily News stories, DiLorenzo wrote celebrity interviews, including in-depth talks with Marilyn Monroe and Elizabeth Taylor. After becoming a stay-at-home Mom, DiLorenzo continued writing a weekly column for the Daily News titled â€œTrips and Treats.â€[4] Following her graduation from Ursuline Highschool in the Bronx, Martin attended Boston College, graduating summa cum laude with a degree in Broadcast Communications and Theater. While at Boston College, she founded Americaâ€™s oldest collegiate improv comedy troupe called My Motherâ€™s Fleabag which boasts a slew of celebrity alumni such as Amy Poehler\",\n",
        "  \"Miller is of African American and Philippine American ancestry and identifies as BIPOC. Her paternal family is from Washington, D.C. and her maternal family is Filipino Creole from the U.S. Virgin Islands. Miller's grandmother was an indigenous Danish west Indian and Ghanaian. Her great-great grandmother is of Ghanaian descent from St. Johns. Her Philippine grandfather was part of the U.S. Filipino navy as a steward's cook during WWI in 1917. Her grandparents met at the USO and married. Having three Filipino lineages of their own, Miller's mother being one of them. Her mother, a Filipino-Creole, came to the United States to attend Howard University. Her maternal Afro-Caribbean Saint-Tomian cousin Larry was installed as a chief of their tribe. Their tribe are artisans, specifically shipbuilders, wood carvers and make decorative coffins. Her paternal grandfather was white and American Indian with Patriotic daughters of American Revolution DNA.[1] Miller's parents met at Howard University in Washington, D.C. where they settled and began the family of their own. She was a Girl Scout and when she won her first award, she was published on the section covers of The Washington Post and Washington Star. As a child, Miller spent most of her days at museums looking at an era of contemporary art and the cadre of Washington color field schools. In 1969, her teacher told her that she will never make it as an artist, but her specific experiences and heritage became formative to her art practice.[1] Miller graduated from Calvin Coolidge High School. In 1985, she received a Masters of Science in Visual Communications at the Pratt Institute.[2] As a thesis project, she was asked to make a contribution to the field of graphic design and instead of a visual design project she wrote Transcending the Problems of the Black Designer to Success in the Marketplace as her thesis.[3][4] Millers 1985 thesis studies design, sociology and history to give a portrait of African American job prospects.[5] She later attended the Rhode Island School of Design but when Miller's father died, she transferred to Maryland Institute College of Art where she lived closer to her mother.[3] Miller was awarded a Doctor of Humane Letters from the Vermont College of Fine Arts in February 2021.[6] In May of 2022, Miller was awarded a Doctor of Fine Arts from Maryland Institute College of Art.[7][8] In June of 2022, Miller was awarded a Doctor of Fine Arts from the Rhode Island School of Design.\",\n",
        "  \"Moscovici was born in Bucharest, Romania. At the age of 12, she immigrated with her family to the United States where she has gone on to obtain a B.A. from Princeton University and a Ph.D. in Comparative Literature from Brown University. Moscovici taught philosophy, literature and arts and ideas at Boston University and at the University of Michigan. Born in Bucharest, Romania, she writes from her experience of life in a totalitarian regime, which marked her deeply.\",\n",
        "  \"Born in Los Angeles, Nishi was the daughter of Hatsu and Tahei Matsunaga who had emigrated to the United States from Kumamoto, Japan.[3] Her father was a hotel owner in the Little Tokyo district of Los Angeles. She attended Theodore Roosevelt High School before enrolling as a music student at the University of Southern California. A trained classical pianist, she often played with her sister Helen who was a violinist. Despite writing a telegram to President Roosevelt complaining about internment as undemocratic she had to interrupt her studies and her music career in spring 1942 when, as Japanese Americans, she and her family were incarcerated at the Santa Anita Assembly Center following the Japanese attack on Pearl Harbor.[1] Five months later, in the fall of 1942, she and her sister were among the first students to leave the internment camp thanks to the efforts of the National Japanese American Student Relocation Council. Nishi then studied sociology at Washington University in St. Louis where she earned a master's degree in 1944. She completed her education at the sociology department of the University of Chicago where she ultimately received a doctorate in 1963.\",\n",
        "  \"Patricia T OConner (born February 19, 1949) is the author of five books about the English language.[1] A former editor at The New York Times Book Review, she appears monthly on WNYC as the word maven for The Leonard Lopate Show.[2] She has written extensively for The New York Times, including On Language columns, book reviews, and articles for the op-ed page and the Week in Review section. She and Stewart Kellerman, her husband and co-author of several books and articles, answer questions about the English language on The Grammarphobia Blog.[3] She graduated from Grinnell College in 1971 with a BA in philosophy, and received an honorary degree from Grinnell in 2006.\",\n",
        "  \"Chaon was born June 11, 1964 in either Sidney, Nebraska[1][2] or Omaha, Nebraska,[3] and was the adopted son of Earl D. Chaon and Teresa N. (Tallmage) Chaon. His father was a construction worker and his mother a stay-at-home mom, neither of whom graduated from high school. He was the oldest of three siblings. He grew up in a village of 20 people outside of Sidney, Nebraska. Chaon has said about his childhood, I was a weird kid: bookish, imaginative, and not athletic. He was a voracious young reader: At 10, I wanted to read The New Yorker. Foreshadowing some themes of his later writing, he noted, I grew up on a steady diet of SF and horror and ghost stories, and thatâ€™s still a love of mine, and that as a teenager he was fascinated by the serial killer novels that were popular in the 1980s. Furthermore, he said, Ive written stories since I was a little kid. He credits his parents for their support: I had parents who, however puzzled they were by my weirdness, were tolerant of it and loving.\",\n",
        "  \"Charney was born in New Haven, Connecticut in 1979. His parent a psychiatrist and a professor of French Literature at Yale University, were, in his wordof the class of Americans who idealize Europe and as a youth he spent most of his summers in France.[1] He attended Choate Rosemary Hall, and received his undergraduate education at Colby College in Maine, where he majored in Art History and English Literature During this period he participated in exchange programs in both Paris and London Also while at Colby he founded the Colby Film Society and wrote several plays, one of which won the Horizons New Young Playwrights Competition in Atlanta, Georgia,[3] in 2002 the year of his graduation. After graduating he moved to London, where he studied at the Courtauld Institute and received a Masters for his work on seventeenth century sculpture in Rome. He subsequently attended Cambridge University S Johns College where he received a second Masters in Historywriting on Bronzino London Allegory and began a PhD, but chose not to complete his thesis. In the fall of 2012, he received a PhD in Art History from the University of Ljubljana, with a thesis on the work of the Slovenian architect JoÅ¾e PleÄnik 1872â€“1957\",\n",
        "  \"Kevin Canty (born January 17, 1953) is an American novelist and short story writer. He is a faculty member in the English department at the University of Montana at Missoula, where he currently resides.[1] Canty received his master's degree in English from the University of Florida in 1990.[2] He received his M.F.A. in creative writing from the University of Arizona in 1993.\",\n",
        "  \"Christopher Robert Cargill (born September 8, 1975) is an American screenwriter, novelist, podcast host, and former film critic known under the pseudonyms Massawyrm (on Ain't It Cool News) and Carlyle (on Spill.com). Cargill currently resides in Austin, Texas with his wife.[1] He is best known for writing the films Sinister (2012), Sinister 2 (2015), Doctor Strange (2016), and The Black Phone (2021). He is a frequent writing collaborator of Scott Derrickson. Cargill was raised in a military family, growing up on army bases around the United States. He held several jobs prior to writing, including video store clerk and travel agent.[citation needed]\",\n",
        "  \"he is Card (born August 24, 1951) is an American writer. He is currently the only person to win both a Hugo Award and a Nebula Award in consecutive years, winning both awards for both his novel Ender's Game (1985) and its sequel Speaker for the Dead (1986) back-to-back. A feature film adaptation of Ender's Game, which Card co-produced, was released in 2013. Card also wrote the Locus Fantasy Award-winning series The Tales of Alvin Maker (1987â€“2003).\"\n",
        "]\n",
        "\n",
        "export_model.predict(examples)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}